{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings con OpenAI\n",
    "Embeddings es un proceso mediante el cual se utiliza alguna tecnica/algoritmo que sea capaz de convertir palabras o texto a vectores de N dimensiones. Estos vectores contienen cierto nivel de información semantica sobre el texto o palabra. Por ejemplo, palabras que son muy similares van a tener valores cercanos en sus representaciones en vectores.\n",
    "\n",
    "Hay varios modelos que son capaces de hacer un embedding de nuestro texto, en este cuaderno estaremos utilizando el embedding de OpenAI, el cual tiene la capacidad de posicionas muy bien palabras o textos segun su semantica. Este es el mismo embedding que utiliza GPT3. En este cuaderno estarás haciendo embedding de un texto para despues poder buscar cosas dentro de este texto por medio de preguntas en lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradioNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/73/0a/792d54defeefbe900140bb56f08b8375f4bbe240ed534a42bb6364989b5d/gradio-3.44.3-py3-none-any.whl.metadata\n",
      "  Downloading gradio-3.44.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Obtaining dependency information for aiofiles<24.0,>=22.0 from https://files.pythonhosted.org/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl.metadata\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio)\n",
      "  Obtaining dependency information for altair<6.0,>=4.2.0 from https://files.pythonhosted.org/packages/f2/b4/02a0221bd1da91f6e6acdf0525528db24b4b326a670a9048da474dfe0667/altair-5.1.1-py3-none-any.whl.metadata\n",
      "  Using cached altair-5.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting fastapi (from gradio)\n",
      "  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/76/e5/ca411b260caa4e72f9ac5482f331fe74fd4eb5b97aa74d1d2806ccf07e2c/fastapi-0.103.1-py3-none-any.whl.metadata\n",
      "  Downloading fastapi-0.103.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.3.1-py3-none-any.whl\n",
      "Collecting gradio-client==0.5.0 (from gradio)\n",
      "  Obtaining dependency information for gradio-client==0.5.0 from https://files.pythonhosted.org/packages/fe/85/ec0323f39192c4bee04e8e06e64213aff816b9d1b61c3c8367e75b1c7e10/gradio_client-0.5.0-py3-none-any.whl.metadata\n",
      "  Using cached gradio_client-0.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx (from gradio)\n",
      "  Obtaining dependency information for httpx from https://files.pythonhosted.org/packages/33/0d/d9ce469af019741c8999711d36b270ff992ceb1a0293f73f9f34fdf131e9/httpx-0.25.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.25.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting huggingface-hub>=0.14.0 (from gradio)\n",
      "  Obtaining dependency information for huggingface-hub>=0.14.0 from https://files.pythonhosted.org/packages/50/9d/5eac2733606df7d164b951b14cd76b056e530af96c881aaec89383bdbe45/huggingface_hub-0.17.1-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio)\n",
      "  Obtaining dependency information for importlib-resources<7.0,>=1.3 from https://files.pythonhosted.org/packages/25/d4/592f53ce2f8dde8be5720851bd0ab71cc2e76c55978e4163ef1ab7e389bb/importlib_resources-6.0.1-py3-none-any.whl.metadata\n",
      "  Using cached importlib_resources-6.0.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jinja2<4.0 (from gradio)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Obtaining dependency information for markupsafe~=2.0 from https://files.pythonhosted.org/packages/be/bb/08b85bc194034efbf572e70c3951549c8eca0ada25363afc154386b5390a/MarkupSafe-2.1.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached MarkupSafe-2.1.3-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting matplotlib~=3.0 (from gradio)\n",
      "  Obtaining dependency information for matplotlib~=3.0 from https://files.pythonhosted.org/packages/40/d9/c1784db9db0d484c8e5deeafbaac0d6ed66e165c6eb4a74fb43a5fa947d9/matplotlib-3.8.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading matplotlib-3.8.0-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting numpy~=1.0 (from gradio)\n",
      "  Obtaining dependency information for numpy~=1.0 from https://files.pythonhosted.org/packages/93/fd/3f826c6d15d3bdcf65b8031e4835c52b7d9c45add25efa2314b53850e1a2/numpy-1.26.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading numpy-1.26.0-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.1/61.1 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Obtaining dependency information for orjson~=3.0 from https://files.pythonhosted.org/packages/b9/60/ffd1debfd00391693922aa998ddadf0cdea73a5f7465a54a29e122d1102b/orjson-3.9.7-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading orjson-3.9.7-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.4/50.4 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in d:\\diegoalonso\\programs\\miniconda\\miniconda3\\envs\\tesis2\\lib\\site-packages (from gradio) (23.1)\n",
      "Collecting pandas<3.0,>=1.0 (from gradio)\n",
      "  Obtaining dependency information for pandas<3.0,>=1.0 from https://files.pythonhosted.org/packages/b7/f8/32d6b5aa4c4bc045fa2c4c58f88c325facc54721956c6313f0afea8ea853/pandas-2.1.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached pandas-2.1.0-cp311-cp311-win_amd64.whl.metadata (18 kB)\n",
      "Collecting pillow<11.0,>=8.0 (from gradio)\n",
      "  Obtaining dependency information for pillow<11.0,>=8.0 from https://files.pythonhosted.org/packages/54/9b/debe992677af84859ec1e38777b1d5c0671918188324153ecbc1f16f6cb6/Pillow-10.0.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading Pillow-10.0.1-cp311-cp311-win_amd64.whl.metadata (9.6 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 (from gradio)\n",
      "  Obtaining dependency information for pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 from https://files.pythonhosted.org/packages/82/06/fafdc75e48b248eff364b4249af4bcc6952225e8f20e8205820afc66e88e/pydantic-2.3.0-py3-none-any.whl.metadata\n",
      "  Using cached pydantic-2.3.0-py3-none-any.whl.metadata (148 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart (from gradio)\n",
      "  Using cached python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "Collecting pyyaml<7.0,>=5.0 (from gradio)\n",
      "  Obtaining dependency information for pyyaml<7.0,>=5.0 from https://files.pythonhosted.org/packages/b3/34/65bb4b2d7908044963ebf614fe0fdb080773fc7030d7e39c8d3eddcd4257/PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting requests~=2.0 (from gradio)\n",
      "  Obtaining dependency information for requests~=2.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting typing-extensions~=4.0 (from gradio)\n",
      "  Obtaining dependency information for typing-extensions~=4.0 from https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Obtaining dependency information for uvicorn>=0.14.0 from https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl.metadata\n",
      "  Using cached uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio)\n",
      "  Using cached websockets-11.0.3-cp311-cp311-win_amd64.whl (124 kB)\n",
      "Collecting fsspec (from gradio-client==0.5.0->gradio)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/6a/af/c673e8c663e17bd4fb201a6f029153ad5d7023aa4442d81c7987743db379/fsspec-2023.9.1-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.9.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6.0,>=4.2.0->gradio)\n",
      "  Obtaining dependency information for jsonschema>=3.0 from https://files.pythonhosted.org/packages/2b/ff/af59fd34bc4d7ac3e6e0cd1f3c10317d329b6c1aee179e8b24ad9a79fbac/jsonschema-4.19.0-py3-none-any.whl.metadata\n",
      "  Using cached jsonschema-4.19.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting toolz (from altair<6.0,>=4.2.0->gradio)\n",
      "  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.14.0->gradio)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.14.0->gradio)\n",
      "  Obtaining dependency information for tqdm>=4.42.1 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib~=3.0->gradio)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/e5/76/94bc17eb868f8c7397f8fdfdeae7661c1b9a35f3a7219da308596e8c252a/contourpy-1.1.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading contourpy-1.1.1-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib~=3.0->gradio)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib~=3.0->gradio)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/95/b6/9a5133deb5838c4dbe3ea27e8dba123622aa5112d43a079e9587636b4faf/fonttools-4.42.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached fonttools-4.42.1-cp311-cp311-win_amd64.whl.metadata (154 kB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib~=3.0->gradio)\n",
      "  Obtaining dependency information for kiwisolver>=1.0.1 from https://files.pythonhosted.org/packages/1e/37/d3c2d4ba2719059a0f12730947bbe1ad5ee8bff89e8c35319dcb2c9ddb4c/kiwisolver-1.4.5-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached kiwisolver-1.4.5-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib~=3.0->gradio)\n",
      "  Obtaining dependency information for pyparsing>=2.3.1 from https://files.pythonhosted.org/packages/39/92/8486ede85fcc088f1b3dba4ce92dd29d126fd96b0008ea213167940a2475/pyparsing-3.1.1-py3-none-any.whl.metadata\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\diegoalonso\\programs\\miniconda\\miniconda3\\envs\\tesis2\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/d8/f0/a2ee543a96cc624c35a9086f39b1ed2aa403c6d355dfe47a11ee5c64a164/annotated_types-0.5.0-py3-none-any.whl.metadata\n",
      "  Using cached annotated_types-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic-core==2.6.3 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio)\n",
      "  Obtaining dependency information for pydantic-core==2.6.3 from https://files.pythonhosted.org/packages/29/37/eb1c5853ecaac79d2dc19be206fd6732fdfa6a6bf705048d0a1046c5fa8d/pydantic_core-2.6.3-cp311-none-win_amd64.whl.metadata\n",
      "  Using cached pydantic_core-2.6.3-cp311-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\diegoalonso\\appdata\\roaming\\python\\python311\\site-packages (from requests~=2.0->gradio) (3.2.0)\n",
      "Collecting idna<4,>=2.5 (from requests~=2.0->gradio)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests~=2.0->gradio)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/9b/81/62fd61001fa4b9d0df6e31d47ff49cfa9de4af03adecf339c7bc30656b37/urllib3-2.0.4-py3-none-any.whl.metadata\n",
      "  Using cached urllib3-2.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests~=2.0->gradio)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting click>=7.0 (from uvicorn>=0.14.0->gradio)\n",
      "  Obtaining dependency information for click>=7.0 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting anyio<4.0.0,>=3.7.1 (from fastapi->gradio)\n",
      "  Obtaining dependency information for anyio<4.0.0,>=3.7.1 from https://files.pythonhosted.org/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl.metadata\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
      "  Obtaining dependency information for starlette<0.28.0,>=0.27.0 from https://files.pythonhosted.org/packages/58/f8/e2cca22387965584a409795913b774235752be4176d276714e15e1a58884/starlette-0.27.0-py3-none-any.whl.metadata\n",
      "  Using cached starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting httpcore<0.19.0,>=0.18.0 (from httpx->gradio)\n",
      "  Obtaining dependency information for httpcore<0.19.0,>=0.18.0 from https://files.pythonhosted.org/packages/ac/97/724afbb7925339f6214bf1fdb5714d1a462690466832bf8fb3fd497649f1/httpcore-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-0.18.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting sniffio (from httpx->gradio)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: colorama in d:\\diegoalonso\\programs\\miniconda\\miniconda3\\envs\\tesis2\\lib\\site-packages (from click>=7.0->uvicorn>=0.14.0->gradio) (0.4.6)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio)\n",
      "  Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/1c/24/83349ac2189cc2435e84da3f69ba3c97314d3c0622628e55171c6798ed80/jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata\n",
      "  Using cached jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio)\n",
      "  Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/be/8e/56d6f1e2d591f4d6cbcba446cac4a1b0dc4f584537e2071d9bcee8eeab6b/referencing-0.30.2-py3-none-any.whl.metadata\n",
      "  Using cached referencing-0.30.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio)\n",
      "  Obtaining dependency information for rpds-py>=0.7.1 from https://files.pythonhosted.org/packages/38/02/66194e83ff01cdd93ac9dfab264d35e8b1dfbb535b6992a3c82fb93c4c1f/rpds_py-0.10.3-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading rpds_py-0.10.3-cp311-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\diegoalonso\\programs\\miniconda\\miniconda3\\envs\\tesis2\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Downloading gradio-3.44.3-py3-none-any.whl (20.2 MB)\n",
      "   ---------------------------------------- 0.0/20.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.7/20.2 MB 15.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.3/20.2 MB 15.9 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.8/20.2 MB 12.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 2.8/20.2 MB 15.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 4.1/20.2 MB 17.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 5.7/20.2 MB 20.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 7.5/20.2 MB 22.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 9.7/20.2 MB 24.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 12.1/20.2 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 13.9/20.2 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 16.0/20.2 MB 43.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 16.9/20.2 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 18.5/20.2 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.2/20.2 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.2/20.2 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 20.2/20.2 MB 27.3 MB/s eta 0:00:00\n",
      "Using cached gradio_client-0.5.0-py3-none-any.whl (298 kB)\n",
      "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached altair-5.1.1-py3-none-any.whl (520 kB)\n",
      "Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 294.8/294.8 kB 17.8 MB/s eta 0:00:00\n",
      "Using cached importlib_resources-6.0.1-py3-none-any.whl (34 kB)\n",
      "Using cached MarkupSafe-2.1.3-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Downloading matplotlib-3.8.0-cp311-cp311-win_amd64.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.4/7.6 MB 28.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.4/7.6 MB 36.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.7/7.6 MB 41.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 44.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.6/7.6 MB 35.0 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.0-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.4/15.8 MB 50.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 4.7/15.8 MB 50.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 6.8/15.8 MB 48.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 9.2/15.8 MB 48.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.1/15.8 MB 46.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.0/15.8 MB 50.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 34.4 MB/s eta 0:00:00\n",
      "Downloading orjson-3.9.7-cp311-none-win_amd64.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.8/134.8 kB ? eta 0:00:00\n",
      "Using cached pandas-2.1.0-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "Downloading Pillow-10.0.1-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.5/2.5 MB 53.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 31.7 MB/s eta 0:00:00\n",
      "Using cached pydantic-2.3.0-py3-none-any.whl (374 kB)\n",
      "Using cached pydantic_core-2.6.3-cp311-none-win_amd64.whl (1.7 MB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Using cached uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "Downloading fastapi-0.103.1-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/66.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 66.2/66.2 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.7/75.7 kB 1.4 MB/s eta 0:00:00\n",
      "Using cached annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "   ---------------------------------------- 0.0/80.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 80.9/80.9 kB 2.3 MB/s eta 0:00:00\n",
      "Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading contourpy-1.1.1-cp311-cp311-win_amd64.whl (480 kB)\n",
      "   ---------------------------------------- 0.0/480.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 480.5/480.5 kB 15.2 MB/s eta 0:00:00\n",
      "Using cached fonttools-4.42.1-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.0/76.0 kB ? eta 0:00:00\n",
      "Using cached jsonschema-4.19.0-py3-none-any.whl (83 kB)\n",
      "Using cached kiwisolver-1.4.5-cp311-cp311-win_amd64.whl (56 kB)\n",
      "Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "   ---------------------------------------- 0.0/103.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 103.1/103.1 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "   ---------------------------------------- 0.0/502.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 502.5/502.5 kB 15.9 MB/s eta 0:00:00\n",
      "Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2023.9.1-py3-none-any.whl (173 kB)\n",
      "   ---------------------------------------- 0.0/173.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 173.4/173.4 kB 10.9 MB/s eta 0:00:00\n",
      "Using cached jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
      "Using cached referencing-0.30.2-py3-none-any.whl (25 kB)\n",
      "Downloading rpds_py-0.10.3-cp311-none-win_amd64.whl (186 kB)\n",
      "   ---------------------------------------- 0.0/186.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 186.4/186.4 kB 11.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, pydub, ffmpy, websockets, urllib3, tzdata, typing-extensions, tqdm, toolz, sniffio, semantic-version, rpds-py, pyyaml, python-multipart, pyparsing, pillow, orjson, numpy, markupsafe, kiwisolver, importlib-resources, idna, h11, fsspec, fonttools, filelock, cycler, click, certifi, attrs, annotated-types, aiofiles, uvicorn, requests, referencing, pydantic-core, pandas, jinja2, contourpy, anyio, starlette, pydantic, matplotlib, jsonschema-specifications, huggingface-hub, httpcore, jsonschema, httpx, fastapi, gradio-client, altair, gradio\n",
      "Successfully installed aiofiles-23.2.1 altair-5.1.1 annotated-types-0.5.0 anyio-3.7.1 attrs-23.1.0 certifi-2023.7.22 click-8.1.7 contourpy-1.1.1 cycler-0.11.0 fastapi-0.103.1 ffmpy-0.3.1 filelock-3.12.4 fonttools-4.42.1 fsspec-2023.9.1 gradio-3.44.3 gradio-client-0.5.0 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.17.1 idna-3.4 importlib-resources-6.0.1 jinja2-3.1.2 jsonschema-4.19.0 jsonschema-specifications-2023.7.1 kiwisolver-1.4.5 markupsafe-2.1.3 matplotlib-3.8.0 numpy-1.26.0 orjson-3.9.7 pandas-2.1.0 pillow-10.0.1 pydantic-2.3.0 pydantic-core-2.6.3 pydub-0.25.1 pyparsing-3.1.1 python-multipart-0.0.6 pytz-2023.3.post1 pyyaml-6.0.1 referencing-0.30.2 requests-2.31.0 rpds-py-0.10.3 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.27.0 toolz-0.12.0 tqdm-4.66.1 typing-extensions-4.7.1 tzdata-2023.3 urllib3-2.0.4 uvicorn-0.23.2 websockets-11.0.3\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos todas las dependencias requeridas, en este caso será Gradio para desarrollar la interfaz grafica y openai para realizar los llamados a su API \n",
    "import gradio as gr\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from openai.embeddings_utils import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la API Key para vincular el cuaderno con nuestra cuenta de OpenAI\n",
    "openai.api_key = \"[INSERTAR AQUI]\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Que es y cómo usar embeddings\n",
    "Al hacer embedding de un dato, lo estamos convirtiendo a un vector numérico, datos similares estarán más cercanos entre si cuando semanticamente son similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se puede hacer embeeding de palabras o cadenas de texto\n",
    "palabras = [\"casa\", \"perro\", \"gato\", \"lobo\", \"leon\", \"zebra\", \"tigre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario = {}\n",
    "for i in palabras:\n",
    "    diccionario[i] = get_embedding(i, engine=\"text-embedding-ada-002\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['casa', 'perro', 'gato', 'lobo', 'leon', 'zebra', 'tigre'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diccionario.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 10 valores de gato:\n",
      " [-0.024549907073378563, 0.00962741393595934, -0.00033602441544644535, -0.011214637197554111, -0.007188035640865564, 0.011865138076245785, -0.021973922848701477, -0.02491418644785881, -0.005207260139286518, -0.01899462752044201]\n",
      "\n",
      "\n",
      "Número de dimensiones del dato embebido\n",
      " 1536\n"
     ]
    }
   ],
   "source": [
    "palabra = \"gato\"\n",
    "print(\"Primeros 10 valores de {}:\\n\".format(palabra), diccionario[palabra][:10])\n",
    "print(\"\\n\")\n",
    "print(\"Número de dimensiones del dato embebido\\n\", len(diccionario[palabra]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar dos embeddings\n",
    "Debido a que los embeddings son una representacion vectorial de los datos en un espacio latente, podemos medir la distancia entre dos vectores y asi obtener que tan similares son. Podemos comparar una palabra nueva o alguna de las que ya fueron embebidas \n",
    "OJO: No necesariamente es similitud al objeto. Ej. perro y gato aun siendo \"opuestos\" semanticamente estan cerca pues tienen una relación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8269982734474424\n"
     ]
    }
   ],
   "source": [
    "n_palabra = \"agujero negro\" # Palabra nueva a comparar\n",
    "palabra_comparar = \"perro\" # Palabra del diccionario con la que compararemos la nueva palabra\n",
    "n_palabra_embed = get_embedding(n_palabra, engine=\"text-embedding-ada-002\")\n",
    "similitud = cosine_similarity(diccionario[palabra_comparar], n_palabra_embed)\n",
    "print(similitud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumar embeddings\n",
    "Como los vectores contienen valores numericos, podemos sumarlos y el resultado será un nuevo vector de un concepto que una los elementos sumados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casa : [0.82807286]\n",
      "perro : [0.85047755]\n",
      "gato : [0.85633048]\n",
      "lobo : [0.85627533]\n",
      "leon : [0.95312838]\n",
      "zebra : [0.95312839]\n",
      "tigre : [0.88062024]\n"
     ]
    }
   ],
   "source": [
    "# Suma dos listas usando pandas\n",
    "sumados = (pd.DataFrame(diccionario[\"leon\"])) + (pd.DataFrame(diccionario[\"zebra\"]))\n",
    "len(sumados)\n",
    "\n",
    "for key, value in diccionario.items():\n",
    "    print(key, \":\", cosine_similarity(diccionario[key], sumados))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicacion de un Chatbot\n",
    "\n",
    "Usaremos Gradio para hacer una interfaz básica donde podremos hacer preguntas y obtendremos una respuesta. \n",
    "Para esto reutilizaremos lo que hemos visto hasta el momento pero usaremos el archivo de **chatbot_qa.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embed_text(path=\"texto.csv\"):\n",
    "    conocimiento_df = pd.read_csv(path)\n",
    "    conocimiento_df['Embedding'] = conocimiento_df['texto'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
    "    conocimiento_df.to_csv('embeddings.csv')\n",
    "    return conocimiento_df\n",
    "\n",
    "def buscar(busqueda, datos, n_resultados=5):\n",
    "    busqueda_embed = get_embedding(busqueda, engine=\"text-embedding-ada-002\")\n",
    "    datos[\"Similitud\"] = datos['Embedding'].apply(lambda x: cosine_similarity(x, busqueda_embed))\n",
    "    datos = datos.sort_values(\"Similitud\", ascending=False)\n",
    "    return datos.iloc[:n_resultados][[\"texto\", \"Similitud\", \"Embedding\"]]\n",
    "\n",
    "texto_emb = embed_text(\"./chatbot_qa.csv\")\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    busqueda = gr.Textbox(label=\"Buscar\")\n",
    "    output = gr.DataFrame(headers=['texto'])\n",
    "    greet_btn = gr.Button(\"Preguntar\")\n",
    "    greet_btn.click(fn=buscar, inputs=[busqueda, gr.DataFrame(texto_emb)], outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesar datos de un PDF\n",
    "Haremos ahora un ejemplo donde leemos un PDF para poder hacer preguntas y traer un exctracto del PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"./mtg.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jasper SandnerC 214/ 269 2/1Oo2\\n™ & © 2014 Wizards of the CoastSP• M15Campo de \\nbatallatú \\n16 vidas\\nrestantes\\nyo\\n18 vidas \\nrestantesCementerio  Biblioteca  \\nBiblioteca  Mano\\nManoCementerio  \\n3Para comenzar el juego, baraja tu mazo, \\ntambién conocido como tu biblioteca. Roba una mano de siete cartas y comprueba cuántas tierras tienes. Puedes mirar la línea de texto que hay bajo la ilustración de cada carta para ver de qué tipo de carta se trata. Para este primer juego, si no tienes al menos dos tierras, baraja de nuevo tu mazo (incluyendo tu mano anterior) y roba una mano nueva.\\nCada jugador comienza con 20 vidas, \\ny cada uno debe llevar la cuenta de su total de vidas de alguna manera (con un dado, lápiz y papel...). ¡Reduce el total de vidas de tu oponente a 0 y ganarás el juego!Comenzar'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un elemento por cada página\n",
    "pages[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto que va a hacer los cortes en el texto\n",
    "split = CharacterTextSplitter(chunk_size=300, separator = '.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 364, which is longer than the specified 300\n",
      "Created a chunk of size 326, which is longer than the specified 300\n",
      "Created a chunk of size 325, which is longer than the specified 300\n",
      "Created a chunk of size 503, which is longer than the specified 300\n",
      "Created a chunk of size 1507, which is longer than the specified 300\n",
      "Created a chunk of size 308, which is longer than the specified 300\n",
      "Created a chunk of size 583, which is longer than the specified 300\n",
      "Created a chunk of size 458, which is longer than the specified 300\n",
      "Created a chunk of size 429, which is longer than the specified 300\n",
      "Created a chunk of size 626, which is longer than the specified 300\n",
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 892, which is longer than the specified 300\n",
      "Created a chunk of size 1311, which is longer than the specified 300\n",
      "Created a chunk of size 333, which is longer than the specified 300\n",
      "Created a chunk of size 1139, which is longer than the specified 300\n",
      "Created a chunk of size 568, which is longer than the specified 300\n",
      "Created a chunk of size 916, which is longer than the specified 300\n",
      "Created a chunk of size 619, which is longer than the specified 300\n",
      "Created a chunk of size 910, which is longer than the specified 300\n",
      "Created a chunk of size 742, which is longer than the specified 300\n",
      "Created a chunk of size 1074, which is longer than the specified 300\n",
      "Created a chunk of size 706, which is longer than the specified 300\n",
      "Created a chunk of size 538, which is longer than the specified 300\n",
      "Created a chunk of size 427, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 867, which is longer than the specified 300\n",
      "Created a chunk of size 644, which is longer than the specified 300\n",
      "Created a chunk of size 447, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 440, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 376, which is longer than the specified 300\n",
      "Created a chunk of size 506, which is longer than the specified 300\n",
      "Created a chunk of size 387, which is longer than the specified 300\n",
      "Created a chunk of size 309, which is longer than the specified 300\n",
      "Created a chunk of size 319, which is longer than the specified 300\n",
      "Created a chunk of size 414, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "textos = split.split_documents(pages) # Lista de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guía de inicio \n",
      "rápidoEdad: 13 o +\n"
     ]
    }
   ],
   "source": [
    "print(textos[0].page_content)\n",
    "#print(textos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\DiegoAlonso\\myfiles\\dev\\vscode\\university\\ciclo_9_tp1\\pruebas-concepto\\embeddings\\embeddings.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DiegoAlonso/myfiles/dev/vscode/university/ciclo_9_tp1/pruebas-concepto/embeddings/embeddings.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Extraemos la parte de page_content de cada texto y lo pasamos a un dataframe\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DiegoAlonso/myfiles/dev/vscode/university/ciclo_9_tp1/pruebas-concepto/embeddings/embeddings.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m textos \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mpage_content) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m textos] \u001b[39m#Lista de parrafos\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DiegoAlonso/myfiles/dev/vscode/university/ciclo_9_tp1/pruebas-concepto/embeddings/embeddings.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m parrafos \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(textos, columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtexto\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DiegoAlonso/myfiles/dev/vscode/university/ciclo_9_tp1/pruebas-concepto/embeddings/embeddings.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(parrafos)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Extraemos la parte de page_content de cada texto y lo pasamos a un dataframe\n",
    "textos = [str(i.page_content) for i in textos] #Lista de parrafos\n",
    "parrafos = pd.DataFrame(textos, columns=[\"texto\"])\n",
    "print(parrafos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "parrafos['Embedding'] = parrafos[\"texto\"].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002')) # Nueva columna con los embeddings de los parrafos\n",
    "parrafos.to_csv('MTG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La misma funcion del chatbot de pregunts y respuestas\n",
    "def embed_text(path=\"texto.csv\"):\n",
    "    conocimiento_df = pd.read_csv(path)\n",
    "    conocimiento_df['Embedding'] = conocimiento_df['texto'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
    "    conocimiento_df.to_csv('mtg-embeddings.csv')\n",
    "    return conocimiento_df\n",
    "\n",
    "def buscar(busqueda, datos, n_resultados=5):\n",
    "    busqueda_embed = get_embedding(busqueda, engine=\"text-embedding-ada-002\")\n",
    "    datos[\"Similitud\"] = datos['Embedding'].apply(lambda x: cosine_similarity(x, busqueda_embed))\n",
    "    datos = datos.sort_values(\"Similitud\", ascending=False)\n",
    "    return datos.iloc[:n_resultados][[\"texto\", \"Similitud\", \"Embedding\"]]\n",
    "\n",
    "texto_emb = parrafos\n",
    "with gr.Blocks() as demo:\n",
    "    busqueda = gr.Textbox(label=\"Buscar\")\n",
    "    output = gr.DataFrame(headers=['texto'])\n",
    "    greet_btn = gr.Button(\"Preguntar\")\n",
    "    greet_btn.click(fn=buscar, inputs=[busqueda, gr.DataFrame(texto_emb)], outputs=output)\n",
    "\n",
    "demo.launch()\n",
    "\n",
    "\n",
    "# resp = buscar(\"Con cuanta vida empiezo?\", parrafos, 5) # Reutilizamos la funcion de \"buscar\" del app de gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp = buscar(\"Con cuanta vida empiezo?\", parrafos, 5) # Reutilizamos la funcion de \"buscar\" del app de gradio\n",
    "# print(resp.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('gpt3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3e99415c7ce2cb9b8857283077fa90c1d7ffec737ddc9f365b1225d7f13a404"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
