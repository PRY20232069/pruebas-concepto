{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTH-oWoga3Vo"
      },
      "source": [
        "# **Paso 0: Gestión de dependencias**\n",
        "### **0.0. Instalar librerias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAZwQCVnx2BU",
        "outputId": "01dea8df-3d0e-4a6f-d187-0e0f3499f439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuXayvc0xvjl"
      },
      "source": [
        "### **0.1. Importar librerias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gzV9cNSa12a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import string\n",
        "import requests\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB51q-LHd8JQ"
      },
      "source": [
        "### **0.2. Configurar openAI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEtiXaR-ePhZ",
        "outputId": "128b309e-04e9-401b-97ce-24763918fe55"
      },
      "outputs": [],
      "source": [
        "# openai.organization = \"\"\n",
        "openai.api_key = \"<api-key>\"\n",
        "%env OPENAI_API_KEY=<api-key>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJHHSgg8aXUM"
      },
      "source": [
        "# **Paso 1: Preparar Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0xNJKY-bsxf"
      },
      "source": [
        "## **1.1. Dataset**\n",
        "En formato .json .csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSI3XOdAc3SH"
      },
      "source": [
        "## **1.2. Convertir a .jsonl**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW8ffIZWcKkR",
        "outputId": "6cba533e-b34b-438e-8bd0-2e5f26e77cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Based on your file extension, your file is formatted as a CSV file\n",
            "- Your file contains 42 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
            "- All prompts end with suffix `\\nResp:`\n",
            "- All prompts start with prefix `1: `\n",
            "- All completions end with suffix ` END`\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Necessary] Your format `CSV` will be converted to `JSONL`\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: y\n",
            "\n",
            "Wrote modified file to `haikus_prepared.jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"haikus_prepared.jsonl\"\n",
            "\n",
            "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\nResp:` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\" END\"]` so that the generated texts ends at the expected place.\n",
            "Once your model starts training, it'll approximately take 3.02 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ],
      "source": [
        "# !openai tools fine_tunes.prepare_data -f respuestas-sarcasticas.json\n",
        "!openai tools fine_tunes.prepare_data -f haikus.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKLQf6LvdVHH"
      },
      "source": [
        "# **Paso 3: Crear modelo fine-tuned**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5SwmSFpek-9",
        "outputId": "9a12ed72-2c7f-41d4-a1e4-2c9eacdb5b66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rUpload progress:   0% 0.00/5.49k [00:00<?, ?it/s]\rUpload progress: 100% 5.49k/5.49k [00:00<00:00, 9.94Mit/s]\n",
            "Uploaded file from haikus_prepared.jsonl: file-Mr4pf7nLRNyGNvWGrHQKhvxj\n",
            "Created fine-tune: ft-IMvvDTCdH6sMFlXGGDdxIDyy\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2023-09-02 07:56:57] Created fine-tune: ft-IMvvDTCdH6sMFlXGGDdxIDyy\n",
            "[2023-09-02 07:57:06] Fine-tune costs $0.22\n",
            "[2023-09-02 07:57:06] Fine-tune enqueued. Queue number: 0\n",
            "[2023-09-02 07:57:07] Fine-tune started\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!openai api fine_tunes.create -t \"haikus_prepared.jsonl\" -m \"davinci\"\n",
        "# !openai api fine_tunes.follow -i <token> #solo se usa cuando se corta el proceso y openai, nos devuelve el <token> para reanudar el proceso de fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmG0QH5_8eLk",
        "outputId": "648d6027-c9f9-41ee-de50-33406a95bf8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7a28589fb0b0> JSON: {\n",
              "  \"object\": \"list\",\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"object\": \"fine-tune\",\n",
              "      \"id\": \"ft-IMvvDTCdH6sMFlXGGDdxIDyy\",\n",
              "      \"hyperparams\": {\n",
              "        \"n_epochs\": 4,\n",
              "        \"batch_size\": 1,\n",
              "        \"prompt_loss_weight\": 0.01,\n",
              "        \"learning_rate_multiplier\": 0.1\n",
              "      },\n",
              "      \"organization_id\": \"org-Fi06OLn1iMDgxfNR42XdZXMK\",\n",
              "      \"model\": \"davinci\",\n",
              "      \"training_files\": [\n",
              "        {\n",
              "          \"object\": \"file\",\n",
              "          \"id\": \"file-Mr4pf7nLRNyGNvWGrHQKhvxj\",\n",
              "          \"purpose\": \"fine-tune\",\n",
              "          \"filename\": \"haikus_prepared.jsonl\",\n",
              "          \"bytes\": 5489,\n",
              "          \"created_at\": 1693641417,\n",
              "          \"status\": \"processed\",\n",
              "          \"status_details\": null\n",
              "        }\n",
              "      ],\n",
              "      \"validation_files\": [],\n",
              "      \"result_files\": [\n",
              "        {\n",
              "          \"object\": \"file\",\n",
              "          \"id\": \"file-zEch50t6DvVIbuPue4VG2hBI\",\n",
              "          \"purpose\": \"fine-tune-results\",\n",
              "          \"filename\": \"compiled_results.csv\",\n",
              "          \"bytes\": 8445,\n",
              "          \"created_at\": 1693641802,\n",
              "          \"status\": \"processed\",\n",
              "          \"status_details\": null\n",
              "        }\n",
              "      ],\n",
              "      \"created_at\": 1693641417,\n",
              "      \"updated_at\": 1693641803,\n",
              "      \"status\": \"succeeded\",\n",
              "      \"fine_tuned_model\": \"davinci:ft-personal-2023-09-02-08-03-21\"\n",
              "    }\n",
              "  ],\n",
              "  \"next_starting_after\": null\n",
              "}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "openai.FineTune.list() #saber que modelos tengo finetuneado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9vGuaR-foFe"
      },
      "source": [
        "# **Paso 4: Usar el modelo fine-tuned**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZc12MhJfsPE"
      },
      "outputs": [],
      "source": [
        "def completion(prompt, engine_id, debug=True):\n",
        "\n",
        "  COMPLETION_ENDPOINT = 'https://api.openai.com/v1/completions'\n",
        "\n",
        "  headers = {'Authorization': 'Bearer {api_key}'.format(api_key=openai.api_key),\n",
        "              'Content-Type': 'application/json'}\n",
        "\n",
        "  data = {\n",
        "      # \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "      \"prompt\": prompt,\n",
        "      \"model\": engine_id,\n",
        "      \"stop\": [\" END\"]\n",
        "  }\n",
        "\n",
        "  response = requests.post(COMPLETION_ENDPOINT, headers=headers, data=json.dumps(data))\n",
        "  result = response.json()\n",
        "\n",
        "  if debug:\n",
        "    print(\"Headers:\")\n",
        "    print(json.dumps(headers, ident=4))\n",
        "    print(\"Data:\")\n",
        "    print(json.dumps(data, ident=4))\n",
        "    print(\"Result:\")\n",
        "    print(json.dumps(result, ident=4))\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    return[x['text'].strip() for x in result['choices']]\n",
        "  else:\n",
        "    return \"Error: {}\".format(result['error']['message'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TeEzAa6gFEI"
      },
      "outputs": [],
      "source": [
        "MY_MODEL = \"davinci:ft-personal-2023-09-02-08-03-21\"\n",
        "MY_PROMPT = \"1: agua\\ncielo\\nmontaña\\nResp:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X9LetmMipqY",
        "outputId": "32f7dfc5-d4e5-4267-a63a-86925b49f1dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['El agua en la quebrada\\nredonda como cielo']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "completion(MY_PROMPT, MY_MODEL, debug=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
